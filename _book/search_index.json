[["setup.html", "1 Setup 1.1 Software. 1.2 Starting Elasticsearch from the Command Line", " 1 Setup The goal of this chapter is to have our Elasticsearch instance up and running. Once we have started Elasticsearch we will be able to start Kibana. Most of our the labs will be executed using the Command Line (CMD, Powershell) and browsing through the interface in Kibana. 1.1 Software. 1.1.1 Download and extract the software in your Home Directory Make sure we have downloaded the required software: Elasticsearch Kibana Logstash version 7.8.0 (Preferably Logstash version 7.11. However, this version has some issues. In that case, go for Logstash version 7.8.0) Filebeat Metricbeat Winlogbeat Make sure you save and extract the files in your Home Directory. This is the directory with your username that contains files like Documents, Downloads, Pictures etc. This directory is usually found in the path My PC &gt; Users &gt; Arie in which Arie is the name of my Home Directory. If you cannot see your Home Directory in your Explorer, enable Hidden Files Unzip each file. You will end up with the following files. Overview of the files in your Home Directory Notes: To keep your Home Directory tidy, you can delete the zip/tar files since we do not need those anymore. In case you have downloaded Logstash 7.8.0, which is advised for these labs, you will have a directory logstash-7.8.0 instead of logstash-7.11.2 as shown in this screenshot. 1.2 Starting Elasticsearch from the Command Line Since we will use Elasticsearch from our own computer, we need to start Elasticsearch manually. We will use the Command Line to execute the script that starts Elasticsearch. In Windows, from the Start menu, open a Command Line window. To find the Command Line we can insert “CMD”. Start the Command Line (Command Prompt) in Windows Navigate to the directory in wich Elasticsearch is located: cd elasticsearch-7.11.2-windows-x86_64/elasticsearch-7.11.2 Use the dir command to get an overview of the content contained in this directory. (elasticsearch-7.11.2-windows-x86_64/elasticsearch-7.11.2) dir Contents of the main elasticsearch directory Note that in this case we passed through another folder to get to the current folder "],["elasticsearch-and-kibana.html", "2 Elasticsearch and Kibana 2.1 Start Elasticsearch 2.2 Start Kibana", " 2 Elasticsearch and Kibana 2.1 Start Elasticsearch Good. From this point we get an overview of all the elements included in our Elasticsearch installation. For now, we are only interested in starting Elasticsearch. The Elasticsourc program, binary, is located in the bin folder. We can start Elasticsearch with the following command: .\\bin\\elasticsearch When Elasticsearch gets started, you will see a similar output as the following screenshot: Elasticsearch started The output from starting Elasticsearch will continue for a little while. For now it is not necessary yet to inspect these output messages: Output Elasticsearch pt.2 When the Elasticsearch stops generating output, when you see a message stating “Node started” (if you are able to find it), Elasticsearch is ready. A traditional way to find out if Elasticsearch is working, we can open our browser and navigate to localhost:9200. This message indicates Elasticsearch is succesfully started Now that our Elasticsearch instance is running, we can start Kibana to get a visual interface for everything we want to in Elasticsearch. 2.2 Start Kibana Like all the other software in these labs, we will start Kibana from the Command Line. Open the a New Command Line window. We do not want to close our Command Line window that has Elasticsearch running. We cannot run Kibana if Elasticsearch is not running. Open a new Command Line window and navigate to the directory containing Kibana. cd kibana-7.11.2-windows-x86_64 Start Kibana just like we have started Elasticsearch in the previous steps: .\\bin\\kibana Just like Elasticsearch, Kibana needs some time to boot. You will see messages indicating that it will look for a running Elasticsearch instance. Based on that, it will return messages like “Yellow” and “Green” which indicates we are ready to use Kibana. While Elasticsearch default runs in port 9200, Kibana runs in port 5601. Open a new browser tab and navigate to localhost:5601. The homepage of Kibana will be displayed. The Kibana homepage "],["metricbeat.html", "3 Metricbeat 3.1 Setup Metricbeat for the first time 3.2 Check out the Metrics Dashboard 3.3 Enable the Metrics Dashboard in Kibana 3.4 Specify Index Patterns 3.5 Checkout our Metrics 3.6 Check out our Metrics in the Dashboard 3.7 Metrics in Dashboard 3.8 Metrics in Discover 3.9 Conclusion", " 3 Metricbeat In the previous lab we have started Elasticsearch and Kibana. In this lab we will setup Metricbeat to send logs of our System to Elasticsearch. When the log files are indexed in Elasticsearch, we can visualize these logs in a Dashboard using Kibana. Several metrics of my currently running laptop 3.1 Setup Metricbeat for the first time When we use Metricbeat for the first time, it needs an initial setup. The setup is specified in a Powershell script called install-service-metricbeat.ps1. We will walk through the steps to install and run metricbeat. Since the initial setup file is a Powershell script, we need to use Powershell instead of the Command Line this time to execute this script. We need to open Powershell as an Administrator in order to have the right permissions for executing scrips. Open Powershell as Administrator: Right-click Run as Administrator Open Powershell as an Administrator Navigate to the directory containing Metricbeat: cd metricbeat-7.11.2-windows-x86_64\\ We can use the dir command to get an overview of the files and folders of the current directory: Contents of the Metricbeat directory If you want to run Metricbeat for the first time, we need to execute the installation script install-service-metricbeat.ps1. Let’s execute this script. .\\install-service-metricbeat.ps1 It is possible you will get an error message “Authorization error”. This is a safety measure which prevents, malicious, scripts from running on our system. We need to lift this restriction for a bit in order to setup Metricbeat: You might stumble upon this Authorization error Insert Y to change the Execution Policy Set-ExecutionPolicy RemoteSigned .\\install-service-metricbeat.ps1 Set-ExecutionPolicy Restricted When the Powershell script is finished, execute the initial setup: .\\metricbeat.exe modules enable system Next step: ./metricbeat setup Metricbeat setup The final step, is ofcourse to start Metric beat. From the moment we start Metricbeat, it will look for System logs in our computer and send those to Elasticsearch to be indexed. To start Metricbeat, run the following command: ./metricbeat -e If we wait for a little while, we will see some log lines with metrics showing up: Logs passing through Metricbeat 3.2 Check out the Metrics Dashboard Now the setup is complete, we will start Metric beat. Metricbeat will find the System Logs on our laptop and index these to Elasticsearch. Let’s move to kibana to get an overview of the data in our Elasticsearch indices. In the Kibana menu select Dev Tools. This will bring us to the editor in which we can execute Elasticsearch requests. Editor in Dev Tools for executing Elasticsearch queries To get an overview of our currnet indices, we can use the following Elasticsearch request: GET _cat/indices?v 3.3 Enable the Metrics Dashboard in Kibana Select Add data in the header Ingest your data “Add data” When we finished the initial setup of Metricbeat, look it up in one of the many types of pre-configured log configurations. Select System metrics The pre-configured System metrics setup Since we already executed the steps (Step 1 - Step 4) as specified on the page, we can proceed by selecting System metrics dashboard Prepare the Metrics Dashboard by selecting System Metric Dashboard 3.4 Specify Index Patterns Before we can checkout our Metrics dashboard, we need to specify an Index Pattern. This will map an Elasticsearch index to a pattern that we can access to display in our visualisations and dashboards in Elasticsearch. Open Stack Management in de Kibana Menu Overview Stack Management This will open the Stack Management menu. Select Index Patterns in the menu on the left. Index patterns Let’s create an Index Pattern for Metricbeat. Insert metric beat in the “index pattern name” field. The field will look for the current indices in Elasticsearch Choose which field is used as the timestamp in our current source of log files. We will select the @timestamp field for this: Select timestamp We get an overview of our succesfully created Index Pattern. Our new Index Pattern 3.5 Checkout our Metrics Strangely we do not see any data yet. Metrics Dashboard no data Let’s hit Refresh button on the right. Metricbeat is continously pushing log-data to Elasticsearch. The Auto Refresh of Kibana will update the visualisations to display the new data. Our Metric data will be visible once we hit the Refresh button Other than refreshing for the latest logs, we can also specify a specific data range: Selecting a data range for the log visualisations Go ahead and explor some different metrics: Different Metrics Metricbeat 3.6 Check out our Metrics in the Dashboard Let’s checkout the Dashboard to get a nice overview of all the Metrics displayed in our dashboard. 3.7 Metrics in Dashboard Select Dashboard in the Kibana menu. Insert Metricbeat System in the search bar. Select the dashboard [Metricbeat System] Overview ECS. This is a pre-configured dashboard which is created during our setup of Metricbeat. As you can see, there are other pre-configured dashboards for different purposes. In our case, we can only use dashboards related to System Metrics since logs of our system are the only type of data we currently have in our Elasticsearch index. Metricbeat System - Overview ECS Dashboard We get an overview of all the systems for which we enabled Metricbeat to log metrics from. In our case this is one system/host, our own computer. An overview of the hosts/systems and their main metrics We can deep dive into the metrics of this system by selecting Host Selected host for the Metrics Dashboard 3.8 Metrics in Discover Select Discover in the Kibana menu. In Discover, we can execute KQL queries for exploring, aggregating or filtering our data. We will do this in Lab 6. Metrics in Discover 3.9 Conclusion That’s it for now. Next time you want to start Metricbeat, you do not have to go through this setup process all over again. You can just start it like Elasticsearch and Kibana. It is also not required anymore to use Powershell since we are not going to run Powershell scripts anymore. "],["winlogbeat.html", "4 Winlogbeat 4.1 Setup Winlogbeat 4.2 Initiate the Windows logs dashboard in Kibana 4.3 Check out the Windows logs in the Elasticsearch index 4.4 Index Patterns en Discover", " 4 Winlogbeat In this lab we will display Windows logs in our Kibana dashboard. The end results will look similar to the following screenshot: Winlogbeat showing Windows logs in Kibana 4.1 Setup Winlogbeat Just like in the labs for Elasticsearch, Kibana and Metricbeat, also Winlogbeat will be started from the Command Line. Make sure Elasticsearch and Kibana are running, both in their seperate Command Line sessions. Open a new Command Line and navigate to the directory where Winlogbeat is located. Start the Command Line (Command Prompt) in Windows cd winlogbeat-7.11.2-windows-x86_64 cd winlogbeat-7.11.2-windows-x86_64 In my case, the main files of Winlogbeat are located in another folder called winlogbeat-7.11.2-windows-x86_64. Therefore the two similar commands. Use the dir command to get an overview of the files and folders in the main directory of Winlogbeat. Files in the main Winlogbeat directory Just like Metricbeat, we need to setup Winlogbeat if this is the first time we use this version on the computer. winlogbeat setup Just like the setup of Metricbeat, the setup of Windows may take a little while. Winlogbeat setup When the setup is finished, we can start Winlogbeat: winlogbeat Start Winlogbeat from the Command Line No that we have started Winlogbeat, we want to initiate our dashboard for Windows logs in Kibana. 4.2 Initiate the Windows logs dashboard in Kibana From the homescreen of Kibana, select the option Add data Add data From the list of pre-configured options select Windows Event Log Windows Event Log from the pre-configured options Since we already executed the options for the (initial) setup of Winlogbeat, we can ignore the steps descbribed. We can verify if Elasticsearch has already received some data of our running Winlogbeat instance in the Command Line. Select the button Check data to let Kibana check if we already have some data. We get a message if Elasticsearch has succesfully received our data. Kibana Winlogbeat check data Next click the SIEM app button to explore our data. Explore data SIEM Congratulations, we are now presented with our Windows logs files in the dashboard. Make sure the filters are properly set. You can also scroll down the page in order to view all the visualisations. When we drill down the different visualisations we also get different tables that show our Windows logs. Windows logs 4.3 Check out the Windows logs in the Elasticsearch index Let’s checkout the raw Windows logs in our Elasticsearch index. In the Kibana menu, navigate to Dev Tools. In the Editor, execute the Elasticsearch request to get an overview of our current indices. GET _cat/indices?v The list with indices contains an index staring with winlogbeat (followed by the timestamp of the initation of the Winlogbeat instance). Let’s checkout what a Window log line indexed in Elasticsearch looks like GET winlogbeat-*/_search { &quot;query&quot;: { &quot;match_all&quot;: {} } } A Windows log indexed in Elasticsearch The _source field contains the original Windows log line. 4.4 Index Patterns en Discover Just like Metricbeat, we have the possiblity to discover the Windows logs provided by Windlogbeat. This requires us to also define an Index Pattern for our winlogbeat index. In see above we can recall these steps. Discover Windows Logs "],["filebeat.html", "5 Filebeat 5.1 Configure filebeat.yml 5.2 Run filebeat", " 5 Filebeat In this lab we are going to use Filebeat to send logs of our running Elasticsearch to Elasticsearch. This sounds a bit confusing. Our running Elasticsearch instance is also logged. These logs are written to a file in our Elasticsearch directory. We are going to use Filebeat to log the content of this file to an index in Elasticsearch. Basically we are going to log our running Elasticsearch instance. 5.1 Configure filebeat.yml Open a new Command Line while at least the Command Line windows of Elasticsearch and Kibana are running. Start the Command Line (Command Prompt) in Windows Navigate to the right directory containing the files and folder for filebeat. cd filebeat-7.11.2-windows-x86_64 Before we start filebeat, we are going to take a look into the filebeat.yml file. This file contains the configuration of our filebeat instance. First we are going to specify a Filebeat stream in which we tell filebeat for which file to look for incoming log lines. The filebeat.yml file contains mainly lines that are commented out # to indicate that the code in that specific line is not active at the moment. Open this file with your favourite Text Editor whether it is in the Command Line or it is an editer like Notepad++ or Sublime Text. I mainly use the latter. We can see these lines as templates that we can use to specify our own configurations. Let’s specify a configuration for an instance in which we are going to ingest the log lines from the Elasticsearch logs file: Filebeat yml Let’s take a look at the configuration we applied: type: As what type of data the ingested data should be interpreted. We choose for log since we are going to ingest log lines of our running Elasticsearch instance. enabled: The current configuration will be enabled. The logs will be ingested as soon as filebeat will be initated. paths: The path in which we can find the log lines we want to log. mutiline.pattern: The pattern that indicates a log line. With a regular expression we indicate the a Date and Time specification indicates that it is the start of a new log line. multiline.negate: Any line that does not match the pattern in multiline.pattern, is part of the previous line. In our case this is true since we want to group all log lines to the current multiline.pattern which is in this case a (regular expression) of a date and time. multiline.match: How filebeat will deal with matching lines. Next to these parameters, there are some other parameters like exclude_files in which we can specify which files to exclude when we specify an entire directory in the paths parameter. For now, we are all set with the current parameters. We will specify/uncomment some additonal important parameters: index index: index: &quot;%{[fields.log_type]}-%{[agent.version]}-%{+yyyy.MM.dd}&quot; templates setup.template.name: “filebeat” setup.template.pattern: &quot;filebeat-*&quot; 5.2 Run filebeat Now that we have finished setting up our Filebeat conifiguration, we are ready to start Filebeat. From the main directory of Filebeat. filebeat Starting Filebeat Now filebeat is running, we must wait for some Elasticsearch log lines which can be found by Filebeat and indexed in Elasticsearch. But waiting can sometimes take a lot of time, so we will make sure Elasticsearch will genereate some logs. In Kibana open Dev Tools to access the editor. Lets put and delete an index. This will be logged by Elasticsearch. Add an index PUT test_for_filebeat Delete tha index again DELETE test_for_filebeat Let’s look at our Command Line in which Elasticsearch is running. The creating and deletion of the index should be logged. Logs of the creation and deletion of the index In a little while, these logs will be indexed in the index we specified in our filebeat.yml file. Our index starts with filebeat GET _cat/indices?v The logs will nog immediately be visible since the logs are not logged realtime are fetched by Filebeat for a specific time interval. Execute the command a few times until you see the new index. If it does not appear, there might be something wrong with the configuration. Elastic logs added to index "],["logstash.html", "6 Logstash 6.1 Ingest Pipelines 6.2 Grok Patterns 6.3 Start Logstash 6.4 Specify Filebeat to ingest to Logstash 6.5 Configure Logstash to ingest with a pipeline 6.6 Check the data from Logstash in Elasticsearch", " 6 Logstash With Filebeat it is possible to ingest log files directly into Elasticsearch. In some situations, we want to modify the logs in different ways before it gets ingested into Elasticsearch. This is where we can use pipelines for. Logstash can be used to specify our pipelines. 6.1 Ingest Pipelines We create a pipeline by nagivating to the Stack Management page in Kibana. Select Ingest Node Pipelines in the menu. Choose Create Pipeline Ingest Pipelines Give the pipeline an explanatory name: Naming and describing the pipeline We are now asked to configure our processor. In this section we need to specify how our pipeline will parse the log data. For this step we can use grok patterns. It is quite hard to figure out what our grok pattern would look like. Fortunately Dev Tools in Elasticsearch contains a debugger in which we can experiment with our incoming data to specify the right grok-pattern. 6.2 Grok Patterns An overview of the grok patterns that are available out-of-the box can be found at Elasticsearch overview grok patterns In Dev Tools open the Grok Debugger tab. Grok Debugger tab To experiment for the right grok pattern, we need an example log line. Copy a log line from the running Elasticsearch instance in the Command Line. For example, use the following: [2021-03-14T15:59:16,860][INFO ][o.e.x.i.IndexLifecycleTransition] [LAPTOP-V6JSAGOU] moving index [filebeat-7.11.2-2021.03.14-000001] from [null] to [{&quot;phase&quot;:&quot;new&quot;,&quot;action&quot;:&quot;complete&quot;,&quot;name&quot;:&quot;complete&quot;}] in policy [filebeat] Paste this line in the Sample Data field. Lets start with some grok patterns: %{GREEDYDATA:details} The GREEDYDATA pattern will take all the that from a certain point. The section after the colons : specifies the name of the field in which we will put this part of the log. Lets extend the grok pattern. Take a few moments to find out how the grok pattern exactly works by trying th following patterns: \\[%{TIMESTAMP_ISO8601:log_timestamp}]%{GREEDYDATA:details} Notice the importance of the position of the spaces. The \\ indicates the [ symbol needs to be “escaped” since it is also used in regular expressions. \\[%{TIMESTAMP_ISO8601:log_timestamp}]\\[%{WORD:log_level} ] Let’s experiment a little further: \\[%{TIMESTAMP_ISO8601:log_timestamp}]\\[%{WORD:log_level}]\\[%{NOTSPACE:log_action}]%{GREEDYDATA:de_rest} The end result: \\[%{TIMESTAMP_ISO8601:log_timestamp}]\\[%{WORD:log_level} ]\\[%{NOTSPACE:log_action}]\\[%{NOTSPACE:node_name}]%{GREEDYDATA:log_details} No that we have figured out the right grok pattern, we can return to the Ingest Node Pipelines page and continue defining our pipeline. [Create a pipeline]((img/screenshot_filebeat_logstash_create_pipeline.PNG) Select Grok in Processors Insert message in Field since this is the original field that contains our log data. Insert the Grok Pattern we just compoosed in the Patterns field. Add the Grok pattern to the pipeline Finish the configuration of the pipeline. 6.3 Start Logstash It is time to start Logstash. Open a new Command Line session and navigate to the main directory of Logstash: cd logstash-7.8.0 Use the dir command to make sure you are in the right directory: The main directory of Logstash Let’s to a little testrun for logstash with the following command: .\\bin\\logstash.bat -f .\\config\\syslog.conf In this command .\\bin\\logstash.bat indicates we want to run logstash With the filename .\\config\\syslog.conf after the -f flag we specify the configuration file in which it is indicated which log-data should be ingested in Elasticsearch. It can take a little while before we see some logs flowing trough the command line. Output of the test run If the test run is succesfully executed, we are sure Logstash works. Our next step is to specify Filebeat to ingest to Logstash instead of directly to Elasticsearch. 6.4 Specify Filebeat to ingest to Logstash Open the filebeat.yml file in which we modified the configurations earlier. Look for the parameter output.logstash and outcomment it which will make it active in the configuration. Also uncomment the line hosts: [&quot;localhost:5044&quot;]. Configure Filebeat to ingest to Logstash 6.5 Configure Logstash to ingest with a pipeline Let’s switch back to Logstash. Getting the logs from Filebeat to Logstash is the first step. The next step is to configure Logstash how to ingest the data in Elasticsearch. To keep it simple, we will use the default logstash_sample.conf configuration (full path from the main Logstash directory is: .\\config\\logstash-sample.conf). Check out the logstash-sample.conf file. The input section specifies where Logstash will look for data, which is in this case Filebeat. Next, output specifies how and where the data will be ingested. This output has several fields: stdout: How would we like to display the progress of the logs that are being ingested by Logstash. With the option rubydebug we will see the all the raw files (in the command line) that are being ingested in Elasticsearch elasticsearch We specify that the logs will be ingested in Elasticsearch hosts: The location of Elasticsearch, which is in this case our instance running locally. index: The name of the index the data will be ingested. If this index does not exist yet, it will be created automatically. pipeline: This is an important step. In the previous section, we have defined our pipeline with the right grok pattern for ingesting the logs in Elasticsearch. In this parameter we need to specify the name of that pipeline. Logstash pipeline Now that we have our Logstash configuration file setup, we are ready to start Logstash to ingest the logs ingested by Filebeat. First we start filebeat (if we still have a Command Line window open with Filebeat ) filebeat In the command line in which we have Logstash, we will start our Logstash instance with our configuration. .\\bin\\logstash.bat -f .\\config\\logstash-sample.conf The startup of Logstash will take little while. It is important that Filebeat is running while we have Logstash running. Otherwise there is no data to be ingested. As soon as Elasticsearch will generate some logs, we will see them flowing through our Logstash instance. Data through Logstash 6.6 Check the data from Logstash in Elasticsearch When our Logstash instance is running, busy ingesting the logs that are being ingested by Filebeat, we can take a look at the data in the new index that we have specified in the Logstash configuration. In Kibana open Devtools en check if our new index is visible. GET _cat/indices?v If the index is visible, we can view what a document in this index looks like. It should match the way we specified it with our grok patterns. GET demo_elastic_logs_clean/_search { &quot;query&quot;: { &quot;match_all&quot;: {} } } Log files in our new index Voila! "],["visualisation.html", "7 Visualisation 7.1 Analyze 7.2 Visualisation", " 7 Visualisation In this lab we are going to analyse and visualise the metrix from our metrixbeat* index. Make sure we have metricbeat running to make sure new log data with metrics will keep flowing in. 7.1 Analyze First let’s start analyzing our log data. In the Kibana menu open Discover. Select Discover in Kibana We are going experiment with some filters in the Kibana Query Language KQL. Insert metricset.name:memory The log lines will be filtered Metric set Also try a numeric filter like: system.memory.actual.used.bytes &gt; 7 This is actually 7 bites… No try 4 Gigabytes: system.memory.actual.used.bytes &gt; 600000000 We can expand one of the log lines check out all the fields of the line. Expand a log line We can easily work with ranges: Filter bytes range We can also inspect the Elasticsearch query behind this filter: Elasticearch query behind filter 7.2 Visualisation We have several options in Kibana to create a visualisation of our data. In the Kibana menu, select Visualize. In the top right corner, select Create visualization. Select the option Lens. This is an editor in which we can easily drag and drop fields at the axes of our visualisation. Choose “Area chart” as option. Put the following fields on the axes: Horizontal axis: @timestamp Vertical axis: process.cpu.pct Break down by: process.name Save this visualisation as we can use it later for our dashboard. Visualisation of our processes "]]
